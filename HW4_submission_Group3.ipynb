{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS&E 349: Homework 4\n",
    "## Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chih-hsuankao/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/scipy/__init__.py:137: UserWarning: NumPy 1.16.5 or above is required for this version of SciPy (detected version 1.16.2)\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict monthly S&P 500 index returns with the following financial variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"financial_variables.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>yyyymm</th>\n",
       "      <th>b/m</th>\n",
       "      <th>tbl</th>\n",
       "      <th>AAA</th>\n",
       "      <th>BAA</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>Rfree</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>corpr</th>\n",
       "      <th>svar</th>\n",
       "      <th>d/p</th>\n",
       "      <th>d/y</th>\n",
       "      <th>e/p</th>\n",
       "      <th>d/e</th>\n",
       "      <th>ret (S&amp;P 500 annualized log return including dividends)</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1927-01-01</td>\n",
       "      <td>0.443706</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.050834</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>-0.011299</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>-2.942374</td>\n",
       "      <td>-2.963349</td>\n",
       "      <td>-2.374773</td>\n",
       "      <td>-0.567601</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>0.040566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1927-02-01</td>\n",
       "      <td>0.428501</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0467</td>\n",
       "      <td>0.0559</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.051682</td>\n",
       "      <td>0.002742</td>\n",
       "      <td>-0.005714</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>-2.979535</td>\n",
       "      <td>-2.932946</td>\n",
       "      <td>-2.430353</td>\n",
       "      <td>-0.549182</td>\n",
       "      <td>0.040566</td>\n",
       "      <td>0.002610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1927-03-01</td>\n",
       "      <td>0.469765</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.046370</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>-2.976535</td>\n",
       "      <td>-2.970053</td>\n",
       "      <td>-2.445079</td>\n",
       "      <td>-0.531456</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.010907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1927-04-01</td>\n",
       "      <td>0.456754</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.050518</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>-2.984225</td>\n",
       "      <td>-2.967143</td>\n",
       "      <td>-2.471309</td>\n",
       "      <td>-0.512916</td>\n",
       "      <td>0.010907</td>\n",
       "      <td>0.057096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1927-05-01</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.055279</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>-3.025963</td>\n",
       "      <td>-2.975058</td>\n",
       "      <td>-2.531446</td>\n",
       "      <td>-0.494518</td>\n",
       "      <td>0.057096</td>\n",
       "      <td>-0.025705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1927-06-01</td>\n",
       "      <td>0.452385</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.058826</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>-0.0069</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>-3.007309</td>\n",
       "      <td>-3.016743</td>\n",
       "      <td>-2.531330</td>\n",
       "      <td>-0.475979</td>\n",
       "      <td>-0.025705</td>\n",
       "      <td>0.081687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1927-07-01</td>\n",
       "      <td>0.414553</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.059754</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>-0.017045</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>-3.061144</td>\n",
       "      <td>-2.998173</td>\n",
       "      <td>-2.603707</td>\n",
       "      <td>-0.457437</td>\n",
       "      <td>0.081687</td>\n",
       "      <td>0.028624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1927-08-01</td>\n",
       "      <td>0.396227</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.054526</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.005780</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>-3.095764</td>\n",
       "      <td>-3.052225</td>\n",
       "      <td>-2.656742</td>\n",
       "      <td>-0.439023</td>\n",
       "      <td>0.028624</td>\n",
       "      <td>0.049095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1927-09-01</td>\n",
       "      <td>0.380586</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0454</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.094617</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>-3.129097</td>\n",
       "      <td>-3.086791</td>\n",
       "      <td>-2.707759</td>\n",
       "      <td>-0.421338</td>\n",
       "      <td>0.049095</td>\n",
       "      <td>-0.048960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1927-10-01</td>\n",
       "      <td>0.413801</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.094370</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>-3.065650</td>\n",
       "      <td>-3.120203</td>\n",
       "      <td>-2.662875</td>\n",
       "      <td>-0.402774</td>\n",
       "      <td>-0.048960</td>\n",
       "      <td>0.064270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1927-11-01</td>\n",
       "      <td>0.379396</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.082270</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>-3.122543</td>\n",
       "      <td>-3.056966</td>\n",
       "      <td>-2.738218</td>\n",
       "      <td>-0.384325</td>\n",
       "      <td>0.064270</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1927-12-01</td>\n",
       "      <td>0.374689</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.076474</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-3.132667</td>\n",
       "      <td>-3.113804</td>\n",
       "      <td>-2.766942</td>\n",
       "      <td>-0.365725</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>-0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1928-01-01</td>\n",
       "      <td>0.378670</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.062605</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0036</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>-3.118894</td>\n",
       "      <td>-3.124003</td>\n",
       "      <td>-2.741324</td>\n",
       "      <td>-0.377570</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>-0.016679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1928-02-01</td>\n",
       "      <td>0.386077</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>-0.011561</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>-3.092631</td>\n",
       "      <td>-3.110432</td>\n",
       "      <td>-2.704291</td>\n",
       "      <td>-0.388340</td>\n",
       "      <td>-0.016679</td>\n",
       "      <td>0.101652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1928-03-01</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.054364</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>-3.186980</td>\n",
       "      <td>-3.084114</td>\n",
       "      <td>-2.788289</td>\n",
       "      <td>-0.398691</td>\n",
       "      <td>0.101652</td>\n",
       "      <td>0.032715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1928-04-01</td>\n",
       "      <td>0.368095</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.049372</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>-3.210431</td>\n",
       "      <td>-3.178535</td>\n",
       "      <td>-2.800832</td>\n",
       "      <td>-0.409599</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.007422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1928-05-01</td>\n",
       "      <td>0.354397</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.047187</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>-0.0077</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>-3.214759</td>\n",
       "      <td>-3.202181</td>\n",
       "      <td>-2.795243</td>\n",
       "      <td>-0.419516</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>-0.038725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1928-06-01</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.050298</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>-0.0024</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>-3.165110</td>\n",
       "      <td>-3.206453</td>\n",
       "      <td>-2.735254</td>\n",
       "      <td>-0.429857</td>\n",
       "      <td>-0.038725</td>\n",
       "      <td>0.008278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1928-07-01</td>\n",
       "      <td>0.360648</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.059380</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0217</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-3.169302</td>\n",
       "      <td>-3.156873</td>\n",
       "      <td>-2.729377</td>\n",
       "      <td>-0.439924</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.065514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1928-08-01</td>\n",
       "      <td>0.324030</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.057398</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>-3.232747</td>\n",
       "      <td>-3.161253</td>\n",
       "      <td>-2.783671</td>\n",
       "      <td>-0.449077</td>\n",
       "      <td>0.065514</td>\n",
       "      <td>0.019937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1928-09-01</td>\n",
       "      <td>0.328166</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.0559</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.027979</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>-3.248318</td>\n",
       "      <td>-3.224642</td>\n",
       "      <td>-2.790435</td>\n",
       "      <td>-0.457882</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.012055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1928-10-01</td>\n",
       "      <td>0.308931</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.034018</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>-0.005780</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>-3.254680</td>\n",
       "      <td>-3.240278</td>\n",
       "      <td>-2.787459</td>\n",
       "      <td>-0.467221</td>\n",
       "      <td>0.012055</td>\n",
       "      <td>0.116704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1928-11-01</td>\n",
       "      <td>0.265526</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.038372</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>-0.0036</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>-3.360085</td>\n",
       "      <td>-3.246823</td>\n",
       "      <td>-2.884377</td>\n",
       "      <td>-0.475709</td>\n",
       "      <td>0.116704</td>\n",
       "      <td>0.002888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1928-12-01</td>\n",
       "      <td>0.259667</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.063068</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>-3.355051</td>\n",
       "      <td>-3.352172</td>\n",
       "      <td>-2.870448</td>\n",
       "      <td>-0.484602</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.053892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1929-01-01</td>\n",
       "      <td>0.245347</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.078448</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>-3.398869</td>\n",
       "      <td>-3.343355</td>\n",
       "      <td>-2.912289</td>\n",
       "      <td>-0.486581</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>-0.008075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1929-02-01</td>\n",
       "      <td>0.245424</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.071782</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0157</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>-3.381464</td>\n",
       "      <td>-3.387308</td>\n",
       "      <td>-2.892954</td>\n",
       "      <td>-0.488509</td>\n",
       "      <td>-0.008075</td>\n",
       "      <td>-0.003213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1929-03-01</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.079803</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>-0.005848</td>\n",
       "      <td>-0.0144</td>\n",
       "      <td>-0.0087</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>-3.367688</td>\n",
       "      <td>-3.370035</td>\n",
       "      <td>-2.876601</td>\n",
       "      <td>-0.491087</td>\n",
       "      <td>-0.003213</td>\n",
       "      <td>0.018234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1929-04-01</td>\n",
       "      <td>0.263397</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.099320</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>-3.372320</td>\n",
       "      <td>-3.356388</td>\n",
       "      <td>-2.879407</td>\n",
       "      <td>-0.492913</td>\n",
       "      <td>0.018234</td>\n",
       "      <td>-0.046101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1929-05-01</td>\n",
       "      <td>0.282775</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0373</td>\n",
       "      <td>0.117985</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>-0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.004760</td>\n",
       "      <td>-3.317413</td>\n",
       "      <td>-3.361147</td>\n",
       "      <td>-2.822717</td>\n",
       "      <td>-0.494696</td>\n",
       "      <td>-0.046101</td>\n",
       "      <td>0.109584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1929-06-01</td>\n",
       "      <td>0.253581</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.116196</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>-0.0046</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>-3.412851</td>\n",
       "      <td>-3.306363</td>\n",
       "      <td>-2.916414</td>\n",
       "      <td>-0.496437</td>\n",
       "      <td>0.109584</td>\n",
       "      <td>0.048186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>1063</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>0.308953</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>-0.008070</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>-3.913894</td>\n",
       "      <td>-3.894345</td>\n",
       "      <td>-3.113629</td>\n",
       "      <td>-0.800265</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>-0.062433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1064</td>\n",
       "      <td>2015-08-01</td>\n",
       "      <td>0.330671</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>-0.009535</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>-3.843190</td>\n",
       "      <td>-3.907814</td>\n",
       "      <td>-3.064273</td>\n",
       "      <td>-0.778916</td>\n",
       "      <td>-0.062433</td>\n",
       "      <td>-0.026258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1065</td>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>0.335612</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>-0.012923</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.001557</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>-3.810347</td>\n",
       "      <td>-3.837146</td>\n",
       "      <td>-3.052980</td>\n",
       "      <td>-0.757368</td>\n",
       "      <td>-0.026258</td>\n",
       "      <td>0.082049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1066</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>0.309414</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.0259</td>\n",
       "      <td>-0.016208</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>-0.0053</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>-3.883210</td>\n",
       "      <td>-3.803490</td>\n",
       "      <td>-3.148001</td>\n",
       "      <td>-0.735209</td>\n",
       "      <td>0.082049</td>\n",
       "      <td>0.000817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1067</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>0.308429</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>-0.017810</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.002111</td>\n",
       "      <td>-0.0065</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-3.876904</td>\n",
       "      <td>-3.876399</td>\n",
       "      <td>-3.164045</td>\n",
       "      <td>-0.712860</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>-0.017076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1068</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>0.313649</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>-0.021611</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>-3.852454</td>\n",
       "      <td>-3.870140</td>\n",
       "      <td>-3.162143</td>\n",
       "      <td>-0.690311</td>\n",
       "      <td>-0.017076</td>\n",
       "      <td>-0.050250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>1069</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.331911</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>-0.020262</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>-3.796644</td>\n",
       "      <td>-3.848712</td>\n",
       "      <td>-3.110423</td>\n",
       "      <td>-0.686222</td>\n",
       "      <td>-0.050250</td>\n",
       "      <td>-0.004457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>1070</td>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>0.330902</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>-0.024023</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>-3.788779</td>\n",
       "      <td>-3.792916</td>\n",
       "      <td>-3.106633</td>\n",
       "      <td>-0.682146</td>\n",
       "      <td>-0.004457</td>\n",
       "      <td>0.065146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>1071</td>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>0.327955</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>-0.022999</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>-3.848969</td>\n",
       "      <td>-3.785064</td>\n",
       "      <td>-3.170885</td>\n",
       "      <td>-0.678084</td>\n",
       "      <td>0.065146</td>\n",
       "      <td>0.002576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>1072</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>0.326321</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-0.023554</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>-0.0053</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>-3.847238</td>\n",
       "      <td>-3.844542</td>\n",
       "      <td>-3.171731</td>\n",
       "      <td>-0.675507</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.015886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>1073</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>0.326072</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>-0.027005</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>-3.858040</td>\n",
       "      <td>-3.842831</td>\n",
       "      <td>-3.185094</td>\n",
       "      <td>-0.672946</td>\n",
       "      <td>0.015886</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>1074</td>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>0.323475</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>-0.028683</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-3.854562</td>\n",
       "      <td>-3.853652</td>\n",
       "      <td>-3.184161</td>\n",
       "      <td>-0.670401</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.035324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>1075</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>0.314661</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>-0.031666</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>-0.001618</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>-3.885320</td>\n",
       "      <td>-3.850329</td>\n",
       "      <td>-3.210865</td>\n",
       "      <td>-0.674455</td>\n",
       "      <td>0.035324</td>\n",
       "      <td>-0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>1076</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>0.315197</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>-0.030725</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>-0.0140</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-3.879885</td>\n",
       "      <td>-3.881104</td>\n",
       "      <td>-3.201425</td>\n",
       "      <td>-0.678459</td>\n",
       "      <td>-0.001396</td>\n",
       "      <td>-0.001033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>1077</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>0.316794</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>-0.032610</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>-0.0119</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>-3.874452</td>\n",
       "      <td>-3.875687</td>\n",
       "      <td>-3.192038</td>\n",
       "      <td>-0.682414</td>\n",
       "      <td>-0.001033</td>\n",
       "      <td>-0.019206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>1078</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>0.319688</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>-0.028997</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.0314</td>\n",
       "      <td>-0.0263</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>-3.849851</td>\n",
       "      <td>-3.869468</td>\n",
       "      <td>-3.152198</td>\n",
       "      <td>-0.697653</td>\n",
       "      <td>-0.019206</td>\n",
       "      <td>0.033007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>1079</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>0.303286</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>-0.027361</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>-0.001555</td>\n",
       "      <td>-0.0599</td>\n",
       "      <td>-0.0510</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>-3.878495</td>\n",
       "      <td>-3.844891</td>\n",
       "      <td>-3.165980</td>\n",
       "      <td>-0.712515</td>\n",
       "      <td>0.033007</td>\n",
       "      <td>0.017569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>1080</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>0.293479</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>-0.025012</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>-3.891597</td>\n",
       "      <td>-3.873560</td>\n",
       "      <td>-3.164580</td>\n",
       "      <td>-0.727017</td>\n",
       "      <td>0.017569</td>\n",
       "      <td>0.018311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>1081</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.291980</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>-0.022562</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>-3.904363</td>\n",
       "      <td>-3.886636</td>\n",
       "      <td>-3.162272</td>\n",
       "      <td>-0.742091</td>\n",
       "      <td>0.018311</td>\n",
       "      <td>0.037052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>1082</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>0.278678</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>-0.018621</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>-3.935949</td>\n",
       "      <td>-3.899426</td>\n",
       "      <td>-3.179154</td>\n",
       "      <td>-0.756795</td>\n",
       "      <td>0.037052</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>1083</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>0.281599</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>-0.016151</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.0040</td>\n",
       "      <td>-0.0062</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>-3.930648</td>\n",
       "      <td>-3.931037</td>\n",
       "      <td>-3.159503</td>\n",
       "      <td>-0.771145</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.009439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>1084</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>0.277870</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>-0.015497</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>-3.933704</td>\n",
       "      <td>-3.924654</td>\n",
       "      <td>-3.156232</td>\n",
       "      <td>-0.777472</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>0.011920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>1085</td>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>0.276969</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>-0.010100</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>-3.939255</td>\n",
       "      <td>-3.927745</td>\n",
       "      <td>-3.155570</td>\n",
       "      <td>-0.783685</td>\n",
       "      <td>0.011920</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>1086</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>0.272545</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.009702</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>-3.938134</td>\n",
       "      <td>-3.933332</td>\n",
       "      <td>-3.148348</td>\n",
       "      <td>-0.789786</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.019162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>1087</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>0.265804</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.013104</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>-3.950586</td>\n",
       "      <td>-3.931422</td>\n",
       "      <td>-3.157754</td>\n",
       "      <td>-0.792832</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>-0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>1088</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>0.265114</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>-0.012138</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>-3.944464</td>\n",
       "      <td>-3.943918</td>\n",
       "      <td>-3.148636</td>\n",
       "      <td>-0.795828</td>\n",
       "      <td>-0.000324</td>\n",
       "      <td>0.018742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>1089</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>0.259706</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.0259</td>\n",
       "      <td>-0.011027</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.005295</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>-0.0037</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>-3.956959</td>\n",
       "      <td>-3.937840</td>\n",
       "      <td>-3.158184</td>\n",
       "      <td>-0.798775</td>\n",
       "      <td>0.018742</td>\n",
       "      <td>0.023006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>1090</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>0.248906</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>-0.012358</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>-0.0013</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>-3.973667</td>\n",
       "      <td>-3.951722</td>\n",
       "      <td>-3.171451</td>\n",
       "      <td>-0.802216</td>\n",
       "      <td>0.023006</td>\n",
       "      <td>0.028860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>1091</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>0.239727</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>-0.012243</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>-3.972170</td>\n",
       "      <td>-3.968457</td>\n",
       "      <td>-3.166560</td>\n",
       "      <td>-0.805610</td>\n",
       "      <td>0.028860</td>\n",
       "      <td>0.009850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>1092</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0.235393</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>-0.019946</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-0.000588</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-4.000753</td>\n",
       "      <td>-3.966987</td>\n",
       "      <td>-3.191796</td>\n",
       "      <td>-0.808957</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     yyyymm       b/m     tbl     AAA     BAA     lty  \\\n",
       "0              1 1927-01-01  0.443706  0.0323  0.0466  0.0561  0.0351   \n",
       "1              2 1927-02-01  0.428501  0.0329  0.0467  0.0559  0.0347   \n",
       "2              3 1927-03-01  0.469765  0.0320  0.0462  0.0554  0.0331   \n",
       "3              4 1927-04-01  0.456754  0.0339  0.0458  0.0548  0.0333   \n",
       "4              5 1927-05-01  0.434783  0.0333  0.0457  0.0550  0.0327   \n",
       "5              6 1927-06-01  0.452385  0.0307  0.0458  0.0555  0.0334   \n",
       "6              7 1927-07-01  0.414553  0.0296  0.0460  0.0555  0.0333   \n",
       "7              8 1927-08-01  0.396227  0.0270  0.0456  0.0548  0.0329   \n",
       "8              9 1927-09-01  0.380586  0.0268  0.0454  0.0542  0.0330   \n",
       "9             10 1927-10-01  0.413801  0.0308  0.0451  0.0538  0.0325   \n",
       "10            11 1927-11-01  0.379396  0.0304  0.0449  0.0535  0.0320   \n",
       "11            12 1927-12-01  0.374689  0.0317  0.0446  0.0532  0.0316   \n",
       "12            13 1928-01-01  0.378670  0.0331  0.0446  0.0535  0.0321   \n",
       "13            14 1928-02-01  0.386077  0.0333  0.0446  0.0533  0.0318   \n",
       "14            15 1928-03-01  0.363255  0.0327  0.0446  0.0532  0.0317   \n",
       "15            16 1928-04-01  0.368095  0.0362  0.0446  0.0533  0.0319   \n",
       "16            17 1928-05-01  0.354397  0.0390  0.0449  0.0542  0.0327   \n",
       "17            18 1928-06-01  0.370300  0.0392  0.0457  0.0555  0.0326   \n",
       "18            19 1928-07-01  0.360648  0.0412  0.0461  0.0558  0.0344   \n",
       "19            20 1928-08-01  0.324030  0.0436  0.0464  0.0561  0.0341   \n",
       "20            21 1928-09-01  0.328166  0.0457  0.0461  0.0559  0.0346   \n",
       "21            22 1928-10-01  0.308931  0.0470  0.0461  0.0558  0.0336   \n",
       "22            23 1928-11-01  0.265526  0.0426  0.0458  0.0555  0.0338   \n",
       "23            24 1928-12-01  0.259667  0.0426  0.0461  0.0560  0.0340   \n",
       "24            25 1929-01-01  0.245347  0.0466  0.0462  0.0563  0.0349   \n",
       "25            26 1929-02-01  0.245424  0.0439  0.0466  0.0566  0.0363   \n",
       "26            27 1929-03-01  0.272300  0.0460  0.0470  0.0579  0.0377   \n",
       "27            28 1929-04-01  0.263397  0.0480  0.0469  0.0580  0.0358   \n",
       "28            29 1929-05-01  0.282775  0.0509  0.0470  0.0580  0.0373   \n",
       "29            30 1929-06-01  0.253581  0.0480  0.0477  0.0594  0.0367   \n",
       "...          ...        ...       ...     ...     ...     ...     ...   \n",
       "1062        1063 2015-07-01  0.308953  0.0003  0.0415  0.0520  0.0263   \n",
       "1063        1064 2015-08-01  0.330671  0.0007  0.0404  0.0519  0.0264   \n",
       "1064        1065 2015-09-01  0.335612  0.0002  0.0407  0.0534  0.0253   \n",
       "1065        1066 2015-10-01  0.309414  0.0002  0.0395  0.0534  0.0259   \n",
       "1066        1067 2015-11-01  0.308429  0.0012  0.0406  0.0546  0.0265   \n",
       "1067        1068 2015-12-01  0.313649  0.0023  0.0397  0.0546  0.0268   \n",
       "1068        1069 2016-01-01  0.331911  0.0026  0.0400  0.0545  0.0236   \n",
       "1069        1070 2016-02-01  0.330902  0.0031  0.0396  0.0534  0.0217   \n",
       "1070        1071 2016-03-01  0.327955  0.0029  0.0382  0.0513  0.0218   \n",
       "1071        1072 2016-04-01  0.326321  0.0023  0.0362  0.0479  0.0223   \n",
       "1072        1073 2016-05-01  0.326072  0.0027  0.0365  0.0468  0.0219   \n",
       "1073        1074 2016-06-01  0.323475  0.0027  0.0350  0.0453  0.0179   \n",
       "1074        1075 2016-07-01  0.314661  0.0030  0.0328  0.0422  0.0175   \n",
       "1075        1076 2016-08-01  0.315197  0.0030  0.0332  0.0424  0.0186   \n",
       "1076        1077 2016-09-01  0.316794  0.0029  0.0341  0.0431  0.0196   \n",
       "1077        1078 2016-10-01  0.319688  0.0033  0.0351  0.0438  0.0220   \n",
       "1078        1079 2016-11-01  0.303286  0.0045  0.0386  0.0471  0.0267   \n",
       "1079        1080 2016-12-01  0.293479  0.0051  0.0406  0.0483  0.0272   \n",
       "1080        1081 2017-01-01  0.291980  0.0051  0.0392  0.0466  0.0278   \n",
       "1081        1082 2017-02-01  0.278678  0.0052  0.0395  0.0464  0.0270   \n",
       "1082        1083 2017-03-01  0.281599  0.0074  0.0401  0.0468  0.0274   \n",
       "1083        1084 2017-04-01  0.277870  0.0080  0.0387  0.0457  0.0265   \n",
       "1084        1085 2017-05-01  0.276969  0.0089  0.0385  0.0455  0.0256   \n",
       "1085        1086 2017-06-01  0.272545  0.0098  0.0368  0.0437  0.0258   \n",
       "1086        1087 2017-07-01  0.265804  0.0107  0.0370  0.0439  0.0262   \n",
       "1087        1088 2017-08-01  0.265114  0.0101  0.0363  0.0431  0.0242   \n",
       "1088        1089 2017-09-01  0.259706  0.0103  0.0363  0.0430  0.0259   \n",
       "1089        1090 2017-10-01  0.248906  0.0107  0.0360  0.0432  0.0261   \n",
       "1090        1091 2017-11-01  0.239727  0.0123  0.0357  0.0427  0.0260   \n",
       "1091        1092 2017-12-01  0.235393  0.0132  0.0351  0.0422  0.0254   \n",
       "\n",
       "          ntis     Rfree      infl     ltr   corpr      svar       d/p  \\\n",
       "0     0.050834  0.002692 -0.011299  0.0075  0.0056  0.000470 -2.942374   \n",
       "1     0.051682  0.002742 -0.005714  0.0088  0.0069  0.000287 -2.979535   \n",
       "2     0.046370  0.002667 -0.005747  0.0253  0.0083  0.000924 -2.976535   \n",
       "3     0.050518  0.002825  0.000000 -0.0005  0.0055  0.000603 -2.984225   \n",
       "4     0.055279  0.002775  0.005780  0.0109 -0.0011  0.000392 -3.025963   \n",
       "5     0.058826  0.002558  0.011494 -0.0069  0.0043  0.000825 -3.007309   \n",
       "6     0.059754  0.002467 -0.017045  0.0050  0.0003  0.000426 -3.061144   \n",
       "7     0.054526  0.002250 -0.005780  0.0076  0.0083  0.001276 -3.095764   \n",
       "8     0.094617  0.002233  0.005814  0.0018  0.0149  0.001123 -3.129097   \n",
       "9     0.094370  0.002567  0.005780  0.0099  0.0055  0.001559 -3.065650   \n",
       "10    0.082270  0.002533 -0.005747  0.0097  0.0068  0.000930 -3.122543   \n",
       "11    0.076474  0.002642  0.000000  0.0072  0.0068  0.000604 -3.132667   \n",
       "12    0.062605  0.002758  0.000000 -0.0036  0.0027  0.000861 -3.118894   \n",
       "13    0.055172  0.002775 -0.011561  0.0061  0.0068  0.000691 -3.092631   \n",
       "14    0.054364  0.002725  0.000000  0.0045  0.0041  0.001381 -3.186980   \n",
       "15    0.049372  0.003017  0.000000 -0.0004  0.0014  0.001488 -3.210431   \n",
       "16    0.047187  0.003250  0.005848 -0.0077 -0.0078  0.001899 -3.214759   \n",
       "17    0.050298  0.003267 -0.005814  0.0041 -0.0024  0.003645 -3.165110   \n",
       "18    0.059380  0.003433  0.000000 -0.0217 -0.0010  0.001675 -3.169302   \n",
       "19    0.057398  0.003633  0.000000  0.0076  0.0083  0.001229 -3.232747   \n",
       "20    0.027979  0.003808  0.011696 -0.0041  0.0030  0.000650 -3.248318   \n",
       "21    0.034018  0.003917 -0.005780  0.0158  0.0083  0.000679 -3.254680   \n",
       "22    0.038372  0.003550  0.000000  0.0003 -0.0036  0.001523 -3.360085   \n",
       "23    0.063068  0.003550 -0.005814  0.0004  0.0084  0.004077 -3.355051   \n",
       "24    0.078448  0.003883  0.000000 -0.0090  0.0043  0.001701 -3.398869   \n",
       "25    0.071782  0.003658  0.000000 -0.0157  0.0030  0.002670 -3.381464   \n",
       "26    0.079803  0.003833 -0.005848 -0.0144 -0.0087  0.003611 -3.367688   \n",
       "27    0.099320  0.004000 -0.005882  0.0275  0.0019  0.002304 -3.372320   \n",
       "28    0.117985  0.004242  0.005917 -0.0162  0.0045  0.004760 -3.317413   \n",
       "29    0.116196  0.004000  0.005882  0.0110 -0.0046  0.001340 -3.412851   \n",
       "...        ...       ...       ...     ...     ...       ...       ...   \n",
       "1062 -0.008070  0.000025  0.000067  0.0329  0.0239  0.001160 -3.913894   \n",
       "1063 -0.009535  0.000058 -0.001416  0.0012 -0.0067  0.005847 -3.843190   \n",
       "1064 -0.012923  0.000017 -0.001557  0.0174  0.0133  0.004053 -3.810347   \n",
       "1065 -0.016208  0.000017 -0.000450 -0.0053  0.0020  0.001619 -3.883210   \n",
       "1066 -0.017810  0.000100 -0.002111 -0.0065  0.0020  0.001119 -3.876904   \n",
       "1067 -0.021611  0.000192 -0.003417 -0.0022  0.0000  0.002839 -3.852454   \n",
       "1068 -0.020262  0.000217  0.001653  0.0476  0.0067  0.004301 -3.796644   \n",
       "1069 -0.024023  0.000258  0.000823  0.0294  0.0232  0.002602 -3.788779   \n",
       "1070 -0.022999  0.000242  0.004306 -0.0003  0.0423  0.001274 -3.848969   \n",
       "1071 -0.023554  0.000192  0.004741 -0.0053  0.0146  0.000818 -3.847238   \n",
       "1072 -0.027005  0.000225  0.004046  0.0082  0.0016  0.000996 -3.858040   \n",
       "1073 -0.028683  0.000225  0.003284  0.0590  0.0377  0.002864 -3.854562   \n",
       "1074 -0.031666  0.000250 -0.001618  0.0081  0.0245  0.000478 -3.885320   \n",
       "1075 -0.030725  0.000250  0.000918 -0.0140  0.0016  0.000279 -3.879885   \n",
       "1076 -0.032610  0.000242  0.002404 -0.0124 -0.0119  0.001673 -3.874452   \n",
       "1077 -0.028997  0.000275  0.001247 -0.0314 -0.0263  0.000364 -3.849851   \n",
       "1078 -0.027361  0.000375 -0.001555 -0.0599 -0.0510  0.000946 -3.878495   \n",
       "1079 -0.025012  0.000425  0.000327 -0.0057  0.0059  0.000524 -3.891597   \n",
       "1080 -0.022562  0.000425  0.005828  0.0043 -0.0021  0.000331 -3.904363   \n",
       "1081 -0.018621  0.000433  0.003146  0.0137  0.0212  0.000224 -3.935949   \n",
       "1082 -0.016151  0.000617  0.000813 -0.0040 -0.0062  0.000563 -3.930648   \n",
       "1083 -0.015497  0.000667  0.002966  0.0145  0.0176  0.000384 -3.933704   \n",
       "1084 -0.010100  0.000742  0.000855  0.0158  0.0226  0.000502 -3.939255   \n",
       "1085 -0.009702  0.000817  0.000907 -0.0010  0.0108  0.000450 -3.938134   \n",
       "1086 -0.013104  0.000892 -0.000690 -0.0027  0.0060  0.000258 -3.950586   \n",
       "1087 -0.012138  0.000842  0.002994  0.0289  0.0153  0.000753 -3.944464   \n",
       "1088 -0.011027  0.000858  0.005295 -0.0205 -0.0037  0.000251 -3.956959   \n",
       "1089 -0.012358  0.000892 -0.000632 -0.0013  0.0076  0.000224 -3.973667   \n",
       "1090 -0.012243  0.001025  0.000024  0.0036  0.0020  0.000361 -3.972170   \n",
       "1091 -0.019946  0.001100 -0.000588  0.0102  0.0256  0.000239 -4.000753   \n",
       "\n",
       "           d/y       e/p       d/e  \\\n",
       "0    -2.963349 -2.374773 -0.567601   \n",
       "1    -2.932946 -2.430353 -0.549182   \n",
       "2    -2.970053 -2.445079 -0.531456   \n",
       "3    -2.967143 -2.471309 -0.512916   \n",
       "4    -2.975058 -2.531446 -0.494518   \n",
       "5    -3.016743 -2.531330 -0.475979   \n",
       "6    -2.998173 -2.603707 -0.457437   \n",
       "7    -3.052225 -2.656742 -0.439023   \n",
       "8    -3.086791 -2.707759 -0.421338   \n",
       "9    -3.120203 -2.662875 -0.402774   \n",
       "10   -3.056966 -2.738218 -0.384325   \n",
       "11   -3.113804 -2.766942 -0.365725   \n",
       "12   -3.124003 -2.741324 -0.377570   \n",
       "13   -3.110432 -2.704291 -0.388340   \n",
       "14   -3.084114 -2.788289 -0.398691   \n",
       "15   -3.178535 -2.800832 -0.409599   \n",
       "16   -3.202181 -2.795243 -0.419516   \n",
       "17   -3.206453 -2.735254 -0.429857   \n",
       "18   -3.156873 -2.729377 -0.439924   \n",
       "19   -3.161253 -2.783671 -0.449077   \n",
       "20   -3.224642 -2.790435 -0.457882   \n",
       "21   -3.240278 -2.787459 -0.467221   \n",
       "22   -3.246823 -2.884377 -0.475709   \n",
       "23   -3.352172 -2.870448 -0.484602   \n",
       "24   -3.343355 -2.912289 -0.486581   \n",
       "25   -3.387308 -2.892954 -0.488509   \n",
       "26   -3.370035 -2.876601 -0.491087   \n",
       "27   -3.356388 -2.879407 -0.492913   \n",
       "28   -3.361147 -2.822717 -0.494696   \n",
       "29   -3.306363 -2.916414 -0.496437   \n",
       "...        ...       ...       ...   \n",
       "1062 -3.894345 -3.113629 -0.800265   \n",
       "1063 -3.907814 -3.064273 -0.778916   \n",
       "1064 -3.837146 -3.052980 -0.757368   \n",
       "1065 -3.803490 -3.148001 -0.735209   \n",
       "1066 -3.876399 -3.164045 -0.712860   \n",
       "1067 -3.870140 -3.162143 -0.690311   \n",
       "1068 -3.848712 -3.110423 -0.686222   \n",
       "1069 -3.792916 -3.106633 -0.682146   \n",
       "1070 -3.785064 -3.170885 -0.678084   \n",
       "1071 -3.844542 -3.171731 -0.675507   \n",
       "1072 -3.842831 -3.185094 -0.672946   \n",
       "1073 -3.853652 -3.184161 -0.670401   \n",
       "1074 -3.850329 -3.210865 -0.674455   \n",
       "1075 -3.881104 -3.201425 -0.678459   \n",
       "1076 -3.875687 -3.192038 -0.682414   \n",
       "1077 -3.869468 -3.152198 -0.697653   \n",
       "1078 -3.844891 -3.165980 -0.712515   \n",
       "1079 -3.873560 -3.164580 -0.727017   \n",
       "1080 -3.886636 -3.162272 -0.742091   \n",
       "1081 -3.899426 -3.179154 -0.756795   \n",
       "1082 -3.931037 -3.159503 -0.771145   \n",
       "1083 -3.924654 -3.156232 -0.777472   \n",
       "1084 -3.927745 -3.155570 -0.783685   \n",
       "1085 -3.933332 -3.148348 -0.789786   \n",
       "1086 -3.931422 -3.157754 -0.792832   \n",
       "1087 -3.943918 -3.148636 -0.795828   \n",
       "1088 -3.937840 -3.158184 -0.798775   \n",
       "1089 -3.951722 -3.171451 -0.802216   \n",
       "1090 -3.968457 -3.166560 -0.805610   \n",
       "1091 -3.966987 -3.191796 -0.808957   \n",
       "\n",
       "      ret (S&P 500 annualized log return including dividends)       ret  \n",
       "0                                             -0.005579        0.040566  \n",
       "1                                              0.040566        0.002610  \n",
       "2                                              0.002610        0.010907  \n",
       "3                                              0.010907        0.057096  \n",
       "4                                              0.057096       -0.025705  \n",
       "5                                             -0.025705        0.081687  \n",
       "6                                              0.081687        0.028624  \n",
       "7                                              0.028624        0.049095  \n",
       "8                                              0.049095       -0.048960  \n",
       "9                                             -0.048960        0.064270  \n",
       "10                                             0.064270        0.014403  \n",
       "11                                             0.014403       -0.009200  \n",
       "12                                            -0.009200       -0.016679  \n",
       "13                                            -0.016679        0.101652  \n",
       "14                                             0.101652        0.032715  \n",
       "15                                             0.032715        0.007422  \n",
       "16                                             0.007422       -0.038725  \n",
       "17                                            -0.038725        0.008278  \n",
       "18                                             0.008278        0.065514  \n",
       "19                                             0.065514        0.019937  \n",
       "20                                             0.019937        0.012055  \n",
       "21                                             0.012055        0.116704  \n",
       "22                                             0.116704        0.002888  \n",
       "23                                             0.002888        0.053892  \n",
       "24                                             0.053892       -0.008075  \n",
       "25                                            -0.008075       -0.003213  \n",
       "26                                            -0.003213        0.018234  \n",
       "27                                             0.018234       -0.046101  \n",
       "28                                            -0.046101        0.109584  \n",
       "29                                             0.109584        0.048186  \n",
       "...                                                 ...             ...  \n",
       "1062                                           0.020101       -0.062433  \n",
       "1063                                          -0.062433       -0.026258  \n",
       "1064                                          -0.026258        0.082049  \n",
       "1065                                           0.082049        0.000817  \n",
       "1066                                           0.000817       -0.017076  \n",
       "1067                                          -0.017076       -0.050250  \n",
       "1068                                          -0.050250       -0.004457  \n",
       "1069                                          -0.004457        0.065146  \n",
       "1070                                           0.065146        0.002576  \n",
       "1071                                           0.002576        0.015886  \n",
       "1072                                           0.015886        0.001002  \n",
       "1073                                           0.001002        0.035324  \n",
       "1074                                           0.035324       -0.001396  \n",
       "1075                                          -0.001396       -0.001033  \n",
       "1076                                          -0.001033       -0.019206  \n",
       "1077                                          -0.019206        0.033007  \n",
       "1078                                           0.033007        0.017569  \n",
       "1079                                           0.017569        0.018311  \n",
       "1080                                           0.018311        0.037052  \n",
       "1081                                           0.037052        0.000055  \n",
       "1082                                           0.000055        0.009439  \n",
       "1083                                           0.009439        0.011920  \n",
       "1084                                           0.011920        0.004364  \n",
       "1085                                           0.004364        0.019162  \n",
       "1086                                           0.019162       -0.000324  \n",
       "1087                                          -0.000324        0.018742  \n",
       "1088                                           0.018742        0.023006  \n",
       "1089                                           0.023006        0.028860  \n",
       "1090                                           0.028860        0.009850  \n",
       "1091                                           0.009850             NaN  \n",
       "\n",
       "[1092 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ret_predictors.csv')\n",
    "df['yyyymm'] = pd.to_datetime(df['yyyymm'], format='%Y%m', errors='coerce').dropna()\n",
    "\n",
    "# shift the targeting y (last column) by 1\n",
    "df['ret'] = df['ret (S&P 500 annualized log return including dividends)'].shift(periods=-1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we are going to use the data from January 1927 to January 1985 as the training set, the data from February 1985 to January 1997 as the validation set and and the data from January 1997 to Nov 2017 as the test set. \n",
    "\n",
    "Report the MSE and R-squared for the training, validation and test data for each of the following methods and briefly interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: (698, 19)\n",
      "Validation Dataset: (144, 19)\n",
      "Test Dataset: (251, 19)\n"
     ]
    }
   ],
   "source": [
    "df = df.set_index(df['yyyymm'])\n",
    "\n",
    "df_train = df['1927-01-01':'1985-02-01']\n",
    "df_validation = df['1985-02-01':'1997-01-01']\n",
    "df_test = df['1997-01-01':].dropna() # drop row with na\n",
    "\n",
    "print('Train Dataset:',df_train.shape)\n",
    "print('Validation Dataset:',df_validation.shape)\n",
    "print('Test Dataset:',df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (698, 15)\n",
      "Train Y shape: (698,)\n",
      "Validation X shape: (144, 15)\n",
      "Validation Y shape: (144,)\n",
      "Test X shape: (251, 15)\n",
      "Test Y shape: (251,)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.loc[:, 'b/m':'d/e'].to_numpy()\n",
    "y_train = df_train['ret'].to_numpy()\n",
    "\n",
    "X_validation = df_validation.loc[:, 'b/m':'d/e'].to_numpy()\n",
    "y_validation = df_validation['ret'].to_numpy()\n",
    "\n",
    "X_test = df_test.loc[:, 'b/m':'d/e'].to_numpy()\n",
    "y_test = df_test['ret'].to_numpy()\n",
    "\n",
    "print('Train X shape:',X_train.shape)\n",
    "print('Train Y shape:',y_train.shape)\n",
    "print('Validation X shape:',X_validation.shape)\n",
    "print('Validation Y shape:',y_validation.shape)\n",
    "print('Test X shape:',X_test.shape)\n",
    "print('Test Y shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 6.14445366e-02 -4.46310302e+04  8.93883395e-01  1.50118456e-01\n",
      " -1.31223497e+00 -5.90874643e-02  5.35571580e+05 -7.59587409e-01\n",
      " -1.15216619e-01  1.61071656e-01  2.34259229e-01  1.67941016e+06\n",
      "  8.62247957e-02 -1.67941027e+06 -1.67941031e+06]\n"
     ]
    }
   ],
   "source": [
    "regr.fit(X_train, y_train)\n",
    "print('Coefficients: \\n', regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_pred_train = regr.predict(X_train)\n",
    "regr_pred_validation = regr.predict(X_validation)\n",
    "regr_pred_test = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.003427\n",
      "R_squared: 0.051918\n",
      "======Validation======\n",
      "MSE: 0.002209\n",
      "R_squared: -0.280197\n",
      "=========Test=========\n",
      "MSE: 0.002390\n",
      "R_squared: -0.300502\n"
     ]
    }
   ],
   "source": [
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_train, regr_pred_train))\n",
    "print('R_squared: %.6f' % r2_score(y_train, regr_pred_train))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_validation, regr_pred_validation))\n",
    "print('R_squared: %.6f' % r2_score(y_validation, regr_pred_validation))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_test, regr_pred_test))\n",
    "print('R_squared: %.6f' % r2_score(y_test, regr_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Penalized Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV #Q2.1\n",
    "from sklearn.linear_model import Ridge, RidgeCV #Q2.2\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV #Q2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let $\\rho$ = 0 (i.e. Lasso), find the optimal $\\lambda$ from the “cross validation” set and report the MSE and $R^2$ in the training, cross validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.0006030459741131299, copy_X=True, fit_intercept=True,\n",
       "   max_iter=10000, normalize=True, positive=False, precompute=False,\n",
       "   random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_validation, y_validation) # cross-validation\n",
    "\n",
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda from cross validation is  0.0006030459741131299\n"
     ]
    }
   ],
   "source": [
    "print('Optimal lambda from cross validation is ', lassocv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pred_train = lasso.predict(X_train)\n",
    "lasso_pred_validation = lasso.predict(X_validation)\n",
    "lasso_pred_test = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.003615\n",
      "R_squared: 0.000000\n",
      "======Validation======\n",
      "MSE: 0.001758\n",
      "R_squared: -0.019107\n",
      "=========Test=========\n",
      "MSE: 0.001838\n",
      "R_squared: -0.000172\n"
     ]
    }
   ],
   "source": [
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_train, lasso_pred_train))\n",
    "print('R_squared: %.6f' % r2_score(y_train, lasso_pred_train))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_validation, lasso_pred_validation))\n",
    "print('R_squared: %.6f' % r2_score(y_validation, lasso_pred_validation))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_test, lasso_pred_test))\n",
    "print('R_squared: %.6f' % r2_score(y_test, lasso_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let $\\rho$ = 1 (i.e. Ridge Regression), find the optimal $\\lambda$ and report the MSE and $R^2$ as part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=None, fit_intercept=True,\n",
       "    gcv_mode=None, normalize=True, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv = RidgeCV(normalize = True)\n",
    "ridgecv.fit(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda from cross validation is  10.0\n"
     ]
    }
   ],
   "source": [
    "print('Optimal lambda from cross validation is ', ridgecv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "ridge_pred_train = ridge.predict(X_train)\n",
    "ridge_pred_validation = ridge.predict(X_validation)\n",
    "ridge_pred_test = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.003584\n",
      "R_squared: 0.008494\n",
      "======Validation======\n",
      "MSE: 0.001767\n",
      "R_squared: -0.024088\n",
      "=========Test=========\n",
      "MSE: 0.001846\n",
      "R_squared: -0.004782\n"
     ]
    }
   ],
   "source": [
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_train, ridge_pred_train))\n",
    "print('R_squared: %.6f' % r2_score(y_train, ridge_pred_train))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_validation, ridge_pred_validation))\n",
    "print('R_squared: %.6f' % r2_score(y_validation, ridge_pred_validation))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_test, ridge_pred_test))\n",
    "print('R_squared: %.6f' % r2_score(y_test, ridge_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal $\\lambda$ and $\\rho$ $(0 \\leq \\rho \\leq 1)$ and report the estimation errors as part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chih-hsuankao/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv='warn', eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,\n",
       "       n_jobs=None, normalize=True, positive=False, precompute='auto',\n",
       "       random_state=None, selection='cyclic', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticcv = ElasticNetCV(normalize = True)\n",
    "elasticcv.fit(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda from cross validation is  0.0012060919482262586\n",
      "Optimal rho from cross validation is  0.5\n"
     ]
    }
   ],
   "source": [
    "print('Optimal lambda from cross validation is ', elasticcv.alpha_)\n",
    "print('Optimal rho from cross validation is ', elasticcv.l1_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic = ElasticNet(alpha = elasticcv.alpha_,  l1_ratio = elasticcv.l1_ratio_, normalize = True)\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "elastic_pred_train = elastic.predict(X_train)\n",
    "elastic_pred_validation = elastic.predict(X_validation)\n",
    "elastic_pred_test = elastic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.003615\n",
      "R_squared: 0.000000\n",
      "======Validation======\n",
      "MSE: 0.001758\n",
      "R_squared: -0.019107\n",
      "=========Test=========\n",
      "MSE: 0.001838\n",
      "R_squared: -0.000172\n"
     ]
    }
   ],
   "source": [
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_train, elastic_pred_train))\n",
    "print('R_squared: %.6f' % r2_score(y_train, elastic_pred_train))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_validation, elastic_pred_validation))\n",
    "print('R_squared: %.6f' % r2_score(y_validation, elastic_pred_validation))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_test, elastic_pred_test))\n",
    "print('R_squared: %.6f' % r2_score(y_test, elastic_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principle Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_X_train = df_train.loc[:, 'b/m':'d/e']\n",
    "pd_y_train = df_train.loc[:, 'ret (S&P 500 annualized log return including dividends)']\n",
    "\n",
    "pd_X_validation = df_validation.loc[:, 'b/m':'d/e']\n",
    "pd_y_validation = df_validation.loc[:, 'ret (S&P 500 annualized log return including dividends)']\n",
    "\n",
    "pd_X_test = df_test.loc[:, 'b/m':'d/e']\n",
    "pd_y_test = df_test.loc[:, 'ret (S&P 500 annualized log return including dividends)']\n",
    "\n",
    "pd_X_train = df_train.loc[:, 'b/m':'d/e']\n",
    "pd_y_train = df_train['ret']\n",
    "\n",
    "pd_X_validation = df_validation.loc[:, 'b/m':'d/e']\n",
    "pd_y_validation = df_validation['ret']\n",
    "\n",
    "pd_X_test = df_test.loc[:, 'b/m':'d/e']\n",
    "pd_y_test = df_test['ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "regr = linear_model.LinearRegression()\n",
    "X_reduced_validation = pca.fit_transform(scale(pd_X_validation))\n",
    "n = len(X_reduced_validation)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*model_selection.cross_val_score(regr, np.ones((n,1)), \n",
    "                                           pd_y_validation.ravel(), \n",
    "                                           cv = kf_10, \n",
    "                                           scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the top 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*model_selection.cross_val_score(regr, \n",
    "                                               X_reduced_validation[:,:i], \n",
    "                                               pd_y_validation.ravel(), \n",
    "                                               cv=kf_10, \n",
    "                                               scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chih-hsuankao/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/matplotlib/axes/_base.py:3215: MatplotlibDeprecationWarning: \n",
      "The `xmin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `left` instead.\n",
      "  alternative='`left`', obj_type='argument')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlim(xmin=-1)\n",
    "plt.xticks(np.arange(0, 20, step=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here, we found that the lowest cross-validation error occurs when num_components=4 are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pcr model on training data \n",
    "pcr = linear_model.LinearRegression()\n",
    "X_reduced_train = pca.fit_transform(scale(pd_X_train))\n",
    "pcr.fit(X_reduced_train[:,:4], pd_y_train)\n",
    "\n",
    "X_reduced_test = pca.fit_transform(scale(pd_X_test))[:,:4]\n",
    "X_reduced_validation = pca.fit_transform(scale(pd_X_validation))[:,:4]\n",
    "\n",
    "# print(X_reduced_train.shape)\n",
    "# print(X_reduced_test.shape)\n",
    "# print(X_reduced_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_pred_train = pcr.predict(X_reduced_train[:,:4])\n",
    "pcr_pred_validation = pcr.predict(X_reduced_validation)\n",
    "pcr_pred_test = pcr.predict(X_reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.003555\n",
      "R_squared: 0.016550\n",
      "======Validation======\n",
      "MSE: 0.001738\n",
      "R_squared: -0.007024\n",
      "=========Test=========\n",
      "MSE: 0.001878\n",
      "R_squared: -0.022258\n"
     ]
    }
   ],
   "source": [
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(pd_y_train, pcr_pred_train))\n",
    "print('R_squared: %.6f' % r2_score(pd_y_train, pcr_pred_train))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(pd_y_validation, pcr_pred_validation))\n",
    "print('R_squared: %.6f' % r2_score(pd_y_validation, pcr_pred_validation))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(pd_y_test, pcr_pred_test))\n",
    "print('R_squared: %.6f' % r2_score(pd_y_test, pcr_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partial Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chih-hsuankao/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/matplotlib/axes/_base.py:3215: MatplotlibDeprecationWarning: \n",
      "The `xmin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `left` instead.\n",
      "  alternative='`left`', obj_type='argument')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 14.65)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(pd_X_validation)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "for i in np.arange(1, 15):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    score = model_selection.cross_val_score(pls, \n",
    "                                            scale(pd_X_validation), \n",
    "                                            pd_y_validation, \n",
    "                                            cv = kf_10, \n",
    "                                            scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(-score)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(np.arange(1, 15), np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlim(xmin=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here, we found that the low cross-validation errors occur when around K=5 partial least squares dimensions are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.003481\n",
      "R_squared: 0.037093\n",
      "======Validation======\n",
      "MSE: 0.001723\n",
      "R_squared: 0.001596\n",
      "=========Test=========\n",
      "MSE: 0.002182\n",
      "R_squared: -0.187290\n"
     ]
    }
   ],
   "source": [
    "pls = PLSRegression(n_components=5)\n",
    "pls.fit(scale(pd_X_train), pd_y_train)\n",
    "\n",
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(pd_y_train, pls.predict(scale(pd_X_train))))\n",
    "print('R_squared: %.6f' % r2_score(pd_y_train, pls.predict(scale(pd_X_train))))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(pd_y_validation, pls.predict(scale(pd_X_validation))))\n",
    "print('R_squared: %.6f' % r2_score(pd_y_validation, pls.predict(scale(pd_X_validation))))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(pd_y_test, pls.predict(scale(pd_X_test))))\n",
    "print('R_squared: %.6f' % r2_score(pd_y_test, pls.predict(scale(pd_X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 2, 'max_leaf_nodes': 30} correpond to L and K respectively\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chih-hsuankao/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  param_grid = {'max_depth': range(1, 10),\n",
    "                                'max_leaf_nodes': range(2, 100, 4)},\n",
    "                  cv = 5, #(default) 5-fold cross validation\n",
    "                  n_jobs = 1,\n",
    "                  scoring = 'neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_validation, y_validation)\n",
    "\n",
    "print(gs.best_params_,'correpond to L and K respectively')\n",
    "#print(-gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor(max_depth = 2,\n",
    "                            max_leaf_nodes = 34) #TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=34, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_pred_train = dtr.predict(X_train)\n",
    "dtr_pred_validation = dtr.predict(X_validation)\n",
    "dtr_pred_test = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.002774\n",
      "R_squared: 0.232629\n",
      "======Validation======\n",
      "MSE: 0.001756\n",
      "R_squared: -0.017647\n",
      "=========Test=========\n",
      "MSE: 0.003811\n",
      "R_squared: -1.074157\n"
     ]
    }
   ],
   "source": [
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_train, dtr_pred_train))\n",
    "print('R_squared: %.6f' % r2_score(y_train, dtr_pred_train))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_validation, dtr_pred_validation))\n",
    "print('R_squared: %.6f' % r2_score(y_validation, dtr_pred_validation))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_test, dtr_pred_test))\n",
    "print('R_squared: %.6f' % r2_score(y_test, dtr_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boosted Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chih-hsuankao/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0001, 'max_depth': 10, 'n_estimators': 1} correpond to v, L and B respectively\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  param_grid = {'max_depth':range(1, 15, 1),\n",
    "                                'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2],\n",
    "                                'n_estimators': range(1, 150, 2)},\n",
    "                  n_jobs = 4, # run in parallel, adjust this with #CPU/#GPU\n",
    "                  scoring = 'neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_validation, y_validation)\n",
    "\n",
    "print(gs.best_params_,'correpond to v, L and B respectively')\n",
    "#print(-gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.001, loss='ls', max_depth=7,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=1,\n",
       "             n_iter_no_change=None, presort='auto', random_state=None,\n",
       "             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brt = GradientBoostingRegressor(max_depth = 7,\n",
    "                                learning_rate = 0.001,\n",
    "                                n_estimators = 1) #TBD\n",
    "brt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "brt_pred_train = brt.predict(X_train)\n",
    "brt_pred_validation = brt.predict(X_validation)\n",
    "brt_pred_test = brt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.003612\n",
      "R_squared: 0.000922\n",
      "======Validation======\n",
      "MSE: 0.001758\n",
      "R_squared: -0.019103\n",
      "=========Test=========\n",
      "MSE: 0.001838\n",
      "R_squared: -0.000301\n"
     ]
    }
   ],
   "source": [
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_train, brt_pred_train))\n",
    "print('R_squared: %.6f' % r2_score(y_train, brt_pred_train))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_validation, brt_pred_validation))\n",
    "print('R_squared: %.6f' % r2_score(y_validation, brt_pred_validation))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_test, brt_pred_test))\n",
    "print('R_squared: %.6f' % r2_score(y_test, brt_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'n_estimators': 2} correpond to L and B respectively\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  param_grid = {'max_depth':range(1, 15, 1),\n",
    "                                'n_estimators': range(1, 150, 1)},\n",
    "                  n_jobs = 4, # run in parallel, adjust this with #CPU/#GPU\n",
    "                  scoring = 'neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_validation, y_validation)\n",
    "\n",
    "print(gs.best_params_,'correpond to L and B respectively')\n",
    "#print(-gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=3, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(max_depth = 5,\n",
    "                            n_estimators = 3) #TBD\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_pred_train = rfr.predict(X_train)\n",
    "rfr_pred_validation = rfr.predict(X_validation)\n",
    "rfr_pred_test = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Train========\n",
      "MSE: 0.002558\n",
      "R_squared: 0.292405\n",
      "======Validation======\n",
      "MSE: 0.003548\n",
      "R_squared: -1.056026\n",
      "=========Test=========\n",
      "MSE: 0.006392\n",
      "R_squared: -2.478741\n"
     ]
    }
   ],
   "source": [
    "print(\"========Train========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_train, rfr_pred_train))\n",
    "print('R_squared: %.6f' % r2_score(y_train, rfr_pred_train))\n",
    "print(\"======Validation======\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_validation, rfr_pred_validation))\n",
    "print('R_squared: %.6f' % r2_score(y_validation, rfr_pred_validation))\n",
    "print(\"=========Test=========\")\n",
    "print('MSE: %.6f' % mean_squared_error(y_test, rfr_pred_test))\n",
    "print('R_squared: %.6f' % r2_score(y_test, rfr_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
